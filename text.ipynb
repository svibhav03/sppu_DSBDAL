{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing\n",
    "methods:Tokenization, POS Tagging, stop words removal, Stemming andLemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and InverseDocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Dr. Devin is learning NLP. It is very interesting and exciting. It is an important area of AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements. The tokens become the input for another process like parsing and text mining. Tokenization is useful because it breaks the text into smaller, more manageable parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Devin is learning NLP.', 'It is very interesting and exciting.', 'It is an important area of AI.']\n",
      "Dr.\n",
      "Devin\n",
      "is\n",
      "learning\n",
      "NLP\n",
      ".\n",
      "It\n",
      "is\n",
      "very\n",
      "interesting\n",
      "and\n",
      "exciting\n",
      ".\n",
      "It\n",
      "is\n",
      "an\n",
      "important\n",
      "area\n",
      "of\n",
      "AI\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) # Sentence Tokenization used to split the text into sentences\n",
    "print(sentences) \n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "Part-of-speech tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. Part-of-speech tagging also known as word classes or lexical categories. The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Devin', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "[('Dr.', 'NNP'), ('Devin', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('NLP', 'NNP'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('very', 'RB'), ('interesting', 'JJ'), ('and', 'CC'), ('exciting', 'VBG'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('important', 'JJ'), ('area', 'NN'), ('of', 'IN'), ('AI', 'NNP'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words Removal\n",
    "Stop words are the most common words in a language like “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at”, etc. Stop words are removed to improve the performance of the model. Stop words are removed to reduce the dimensionality of the data and remove noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'most', \"weren't\", 'should', 'them', 'when', 'no', \"you're\", 'haven', 'do', 'did', 'my', 'too', 'in', 'hasn', \"didn't\", 'up', 'for', 'hadn', 'won', 'yourself', 'they', 'were', 'a', 'your', 'll', 'yourselves', 'had', \"it's\", 'having', 'few', 's', 're', \"that'll\", 'both', 'have', 'just', \"shouldn't\", 'until', 'm', 'further', 'himself', 'where', 'o', 'aren', 'so', 't', 'because', 'theirs', 'from', 'weren', \"mustn't\", 'wouldn', 'these', 'are', \"you've\", 'during', 'him', 'mustn', 'very', 'will', 'whom', 'before', 'once', \"hasn't\", 'through', 'all', \"doesn't\", \"haven't\", 'he', 'at', 'wasn', 'of', 'doing', 'down', 'other', 'out', 'ain', \"isn't\", 'needn', 'on', 'being', 'to', 'has', 'i', 'doesn', \"you'll\", 'some', 'an', 'above', 'which', 'you', 'be', 'itself', 'about', \"won't\", 'who', 'with', \"couldn't\", 'not', 'hers', 'yours', 'is', 'more', 'than', 'herself', 'y', 'our', 'didn', \"wasn't\", 'as', \"shan't\", 'over', 'and', 'd', 'themselves', 'below', \"needn't\", 'their', 'why', 'only', 'there', 've', 'was', 'same', 'what', 'now', 'myself', \"aren't\", 'ma', 'ourselves', 'isn', 'her', 'under', 'it', 'then', \"hadn't\", 'me', \"you'd\", 'here', 'such', 'can', 'any', 'does', 'how', \"she's\", 'am', 'those', 'shouldn', 'each', 'into', 'that', 'mightn', 'don', \"mightn't\", 'or', 'if', 'been', 'after', 'his', \"wouldn't\", 'while', 'nor', 'the', 'between', 'she', 'off', 'shan', \"should've\", 'but', 'we', 'its', 'this', 'again', 'against', 'by', 'own', 'couldn', \"don't\", 'ours'}\n",
      "['Dr.', 'Devin', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "['Dr.', 'Devin', 'learning', 'NLP', '.', 'interesting', 'exciting', '.', 'important', 'area', 'AI', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop words removal\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "    if word.lower() not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Example: The stem of the word working => work. The stem of the word worked => work. The stem of the word works => work. \n",
    "\n",
    "It just removes the suffixes from the word and reduces it to its root word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Devin', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "Dr. : dr.\n",
      "Devin : devin\n",
      "is : is\n",
      "learning : learn\n",
      "NLP : nlp\n",
      ". : .\n",
      "It : it\n",
      "is : is\n",
      "very : veri\n",
      "interesting : interest\n",
      "and : and\n",
      "exciting : excit\n",
      ". : .\n",
      "It : it\n",
      "is : is\n",
      "an : an\n",
      "important : import\n",
      "area : area\n",
      "of : of\n",
      "AI : ai\n",
      ". : .\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} : {ps.stem(word)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "Example: ate => eat, gone => go, are => be, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Devin', 'is', 'learning', 'NLP', '.', 'It', 'is', 'very', 'interesting', 'and', 'exciting', '.', 'It', 'is', 'an', 'important', 'area', 'of', 'AI', '.']\n",
      "Dr. : Dr.\n",
      "Devin : Devin\n",
      "is : be\n",
      "learning : learn\n",
      "NLP : NLP\n",
      ". : .\n",
      "It : It\n",
      "is : be\n",
      "very : very\n",
      "interesting : interest\n",
      "and : and\n",
      "exciting : excite\n",
      ". : .\n",
      "It : It\n",
      "is : be\n",
      "an : an\n",
      "important : important\n",
      "area : area\n",
      "of : of\n",
      "AI : AI\n",
      ". : .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = nltk.word_tokenize(text)\n",
    "print(words)\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word} : {lemmatizer.lemmatize(word, pos='v')}\") # pos='v' is used to specify the part of speech of the word\n",
    "    # If the part of speech is not specified, it will consider the word as a noun by default "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "- Term Frequency (TF) is a measure of how frequently a term occurs in a document. It is calculated by dividing the number of times a word appears in a document by the total number of words in the document.\n",
    "\n",
    "- Inverse Document Frequency (IDF) is a measure of how important a term is. It is calculated by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n",
    "\n",
    "- TF-IDF is the product of TF and IDF. It is used to measure the importance of a term in a document relative to a collection of documents.\n",
    "\n",
    "TF = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "TF-IDF = TF * IDF\n",
    "\n",
    "More the TF-IDF value, more important the word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "and : 0.0\n",
      "document : 0.46979138557992045\n",
      "first : 0.5802858236844359\n",
      "is : 0.38408524091481483\n",
      "one : 0.0\n",
      "second : 0.0\n",
      "the : 0.38408524091481483\n",
      "third : 0.0\n",
      "this : 0.38408524091481483\n",
      "\n",
      "\n",
      "Document 2\n",
      "and : 0.0\n",
      "document : 0.6876235979836938\n",
      "first : 0.0\n",
      "is : 0.281088674033753\n",
      "one : 0.0\n",
      "second : 0.5386476208856763\n",
      "the : 0.281088674033753\n",
      "third : 0.0\n",
      "this : 0.281088674033753\n",
      "\n",
      "\n",
      "Document 3\n",
      "and : 0.511848512707169\n",
      "document : 0.0\n",
      "first : 0.0\n",
      "is : 0.267103787642168\n",
      "one : 0.511848512707169\n",
      "second : 0.0\n",
      "the : 0.267103787642168\n",
      "third : 0.511848512707169\n",
      "this : 0.267103787642168\n",
      "\n",
      "\n",
      "Document 4\n",
      "and : 0.0\n",
      "document : 0.46979138557992045\n",
      "first : 0.5802858236844359\n",
      "is : 0.38408524091481483\n",
      "one : 0.0\n",
      "second : 0.0\n",
      "the : 0.38408524091481483\n",
      "third : 0.0\n",
      "this : 0.38408524091481483\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "unique_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF values of the words in the corpus\n",
    "for i in range(len(corpus)):\n",
    "    print(f\"Document {i+1}\")\n",
    "    for j in range(len(unique_words)):\n",
    "        print(f\"{unique_words[j]} : {X[i,j]}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
